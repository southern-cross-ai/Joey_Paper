# 1.2 Background: Southern Cross AI and the Initial Fine-Tuning Plan

Southern Cross AI began as a civic-minded, open-source initiative with a practical goal: to create a language model that could better reflect Australian language, culture, and institutions. Our initial plan was to fine-tune an existing open-weight model, such as LLaMA or Mistral, using a curated corpus of Australian data. The assumption was simple: if we gathered enough local content, we could produce a version of a model that “sounded Australian” without needing to build from scratch.

To support this, we undertook extensive data collection and cleaning efforts. These included:
- **Literary and public domain texts:** Project Gutenberg Australia and custom-chunked variants
- **Government and academic content:** Wiki-Australia, Australian Women's Register (AWR), ArXiv Australia, and Hansard-style transcripts
- **Corpora of spoken and regional English:** ICE-AUS, COOEE, CoANZSE, and the Australian Corpus of English (ACE)
- **Informal and social media content:** Reddit posts, YouTube comments, Twitter datasets (including election-related tweets and regional conversations)
- **Broadcast and conversational sources:** Australian Radio Talkback Corpus (ART)
- **Filtered web-scale content:** Subsets of Hugging Face’s FineWeb and Common Crawl restricted to .au domains

These datasets represented a diverse and uniquely Australian linguistic landscape. However, we soon discovered that even large volumes of high-quality, regionally grounded data were insufficient. The limitations were architectural, not just informational.

Our early experiments included fine-tuning Mistral-7B using Hugging Face tools primarily to test tooling and prompt alignment, but these remained peripheral. The real turning point was the creation of BabyJoey, our first serious attempt to build a foundational model. Based on a GPT-2-scale transformer with six decoder blocks, BabyJoey was trained on our curated corpus, including the Gutenberg Australia dataset.

Despite its modest scale, BabyJoey's tendency to speak with an outdated Australian tone, almost like a character from a 1960s bush novel or historical radio play, BabyJoey struggled to generate coherent responses. The limited model size meant it often rambled, repeated itself, or drifted off-topic, sounding more like an eccentric period drama than a usable assistant. 

This wasn’t unexpected. Small models rarely generalise well; they lack the depth and contextual capacity to produce consistent or meaningful outputs. But it confirmed firsthand what we suspected: solving for cultural alignment, scalability, and efficiency would require complete control over the architecture, training process, and deployment environment, not just the data.

This marked a key inflection point for Southern Cross AI: a shift away from adapting pre-trained models and toward building foundational systems from the ground up for Australian needs.

*Since this work began, the community has watched fine‑tuning give way to radically more efficient training pipelines. 2024‑25 results from QLoRA‑2 and Gemini‑Sparse show that with 4‑bit quantisation and low‑rank adaptation, the cost of exploring thousands of task‑specific checkpoints has fallen by an order of magnitude.  For Southern Cross AI this validated the early “fine‑tune first” instinct—yet also surfaced a hard ceiling on model capability when base weights remain closed or foreign‑owned.

Today, the principal lesson is strategic rather than technical: **fine‑tuning is a bridge, not a destination.** It lets a small sovereign team stand up demo tasks quickly, build evaluation pipelines, and recruit volunteers around a tangible artefact.  But the bridge ends the moment you need governance guarantees (e.g. supply‑chain assurance) or deep architectural customisation (e.g. incorporating Aboriginal language segmentation modules).  This realisation set the stage for JoeyLLM’s shift to a fully sovereign base model.*
