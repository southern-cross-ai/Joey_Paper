In the past few years, the field of large language models (LLMs) has been shaped by a wave of high-profile releases — GPT-3 and GPT-4 from OpenAI, Meta’s LLaMA family, Falcon from the UAE, and BLOOM from BigScience. These models pushed scale and architecture forward, but most were trained on broadly sourced global data with limited consideration for regional representation or cultural specificity. For countries like Australia, this leads to foundational misalignment: even powerful models may lack relevance to local language, values, and public datasets.

Across the world, new initiatives have emerged with more explicit or implicit ties to national context. Canada has launched its own sovereign AI efforts, while the BLOOM project — although multinational — emphasized linguistic inclusion over geographic grounding. Europe’s Mistral, though framed as a technically driven open-weight model, has become a de facto European flagship. Mistral’s open release of the Mistral 7B and Mixtral 8x7B models, combined with top-tier performance on benchmarks like MMLU and GSM8K, reflects Europe’s broader pursuit of digital sovereignty and open infrastructure.

In China, DeepSeek exemplifies how sovereign LLMs can blend technical advancement with national strategy. The project has produced powerful bilingual models (Chinese and English), released variants like DeepSeek-Coder and DeepSeek-MoE, and aligned itself with China’s push toward foundational AI self-reliance. These efforts, while not always branded as “sovereign,” clearly serve regional needs and long-term national interests.

Even in the Middle East, Falcon has symbolized the UAE’s technological ambition, and recent Arabic-focused efforts have emerged that blend cultural identity with technical relevance. Each of these examples — Mistral, DeepSeek, Falcon, BLOOM — reflects a pattern: the desire not just to consume AI, but to shape it from the ground up.

JoeyLLM was born from a similar recognition: that Australia’s needs will not be met by fine-tuning someone else’s model. While our project began with modest experiments in adaptation, it soon became clear that true alignment and long-term sovereignty required upstream control over data pipelines, model architecture, and training processes.

But JoeyLLM is more than a model. It is also a community. In the early days of OpenAI, EleutherAI, and Hugging Face, much of the momentum came from open collaboration and shared purpose. Over time, many of those organizations have drifted toward commercial models or closed ecosystems. 

Southern Cross AI aims to revive that early spirit — not just through technical transparency, but by fostering an open, engaged, and sovereign community around AI development. Our contributors include students, researchers, open-source volunteers, and public institutions working together on infrastructure and ideas that reflect Australia’s context.

In that sense, JoeyLLM is not just technically distinct. It is culturally distinct — part of a broader movement reclaiming AI as a public good, rooted in national values and shaped by the communities it serves.
