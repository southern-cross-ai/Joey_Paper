# 3.1 Fine-Tuning as the Starting Point

At the outset, Southern Cross AI operated under a common assumption that, with enough targeted data, an existing large language model could be fine-tuned to produce culturally relevant and locally grounded outputs for Australian use cases. The early plan was not to build a model from scratch, but to take advantage of open-weight models like LLaMA or Mistral, and adapt them using standard instruction tuning techniques and region-specific corpora. This was seen as the fastest, most pragmatic route to producing an "Australian" LLM suitable for experimentation, downstream integration, and public sector applications.

This plan also made sense for a newly forming community. Many of our earliest contributors, including academic researchers, independent AI practitioners, and student volunteers, saw fine-tuning as a manageable entry point. At that stage, we had not yet secured interest from major government or industry stakeholders. The work was being done by those already close to the open-source ecosystem: people who could quickly get involved and test ideas using the available tools.

