# 6.3 Preparing for Future Architectures: Mixture of Experts and Beyond

One of our long-term goals is to scale JoeyLLM using Mixture of Experts (MoE) architectures, inspired by models like DeepSeekMoE and Mixtral. These models activate only a subset of their parameters during inference, allowing for efficient scaling without full-compute overhead.
However, preparing for MoE requires foundational understanding: routing mechanisms, expert balancing, and auxiliary loss management. We are currently studying sparse activation strategies and auxiliary-loss-free gating mechanisms, with the intention of implementing them in future JoeyLLM variants.
Weâ€™ve also explored the tooling that supports these approaches such as DeepSpeed, FlashAttention, and fused optimizer libraries which are necessary for managing memory and performance at scale. But integrating these into a model that can still run on Australian hardware, or be deployed offline, remains an ongoing challenge.
