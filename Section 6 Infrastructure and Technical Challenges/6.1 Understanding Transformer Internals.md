# 6.1 Understanding Transformer Internals

Building BabyJoey gave us hands-on exposure to the internals of transformer-based architectures. While much of the current open-source ecosystem abstracts these details, our goal was to understand how the system works beneath the surface from attention mechanisms and layer normalization to optimizer scheduling and token routing.
We worked directly with low-level components in PyTorch, deliberately avoiding overreliance on high-level training frameworks. This helped us map out the full training stack: not just what goes into the model, but how data flows through it, how memory bottlenecks arise, and how inference behavior emerges from weight configuration.
This process allowed us to build technical literacy within the team and among students and volunteers about what actually makes a language model tick. It was an essential step in building the capacity to scale up.
