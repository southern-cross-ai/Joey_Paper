# 10.2 Mixture of Experts Exploration

Beyond dense scaling, the team is actively researching Mixture of Experts (MoE) architectures for future JoeyLLM variants. Inspired by recent work from DeepSeek, Mistral, and Google, MoE architectures allow for large model capacity while activating only a fraction of parameters during inference improving compute efficiency without compromising performance.
This line of work includes:
Implementing auxiliary-loss-free gating
Evaluating 2-of-N expert routing strategies
Benchmarking MoE models for edge and cluster deployment trade-offs
The goal is to deliver a Joey variant with the benefits of large-scale representation power while retaining compatibility with modest Australian infrastructure.
