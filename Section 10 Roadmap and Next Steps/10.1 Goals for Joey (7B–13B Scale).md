# 10.1 Goals for Joey (7B–13B Scale)

The next major milestone is the training of JoeyLLM at full scale, targeting a dense transformer architecture in the 7 to 13 billion parameter range. This model will serve as the first high-capacity, open-weight, Australian foundational model suitable for instruction tuning, downstream customization, and real-world inference.
This version of Joey is being designed with:
Longer context windows (4K–16K) for document reasoning
Instruction tuning support, including domain-specific prompt sets
Flexible inference pathways, enabling ONNX export, quantization, and air-gapped deployment
Training optimizations, including the use of fused ops and mixed precision
The model will build on the groundwork laid by BabyJoey, but with dramatically expanded data coverage, updated preprocessing pipelines, and infrastructure-aware architectural decisions.
