# 4.1 Technical Overview

BabyJoey was the first foundational model trained by Southern Cross AI a deliberately small-scale prototype designed to validate our training pipeline and test end-to-end processes for Australian-specific LLM development. The model architecture was intentionally minimal:
BabyJoey was implemented using the PyTorch library and trained on a single GPU using local compute. The dataset consisted solely of a custom-processed version of Project Gutenberg Australia. For tokenization, we used OpenAIâ€™s cl100k_base vocabulary via the tiktoken library. While this tokenizer was not trained on Australian-specific content, it allowed us to rapidly test our training pipeline using a well-optimized subword vocabulary. Preprocessing included de-duplication, formatting, and length-based chunking.
The goal was not model performance, but infrastructure validation. We set out to confirm that we could ingest data, tokenize effectively, configure a working architecture, run a complete training cycle, and generate outputs entirely on Australian-controlled infrastructure.
