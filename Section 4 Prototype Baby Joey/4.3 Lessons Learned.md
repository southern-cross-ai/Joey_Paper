# 4.3 Lessons Learned

Despite its limitations, BabyJoey gave us critical insights:

* Small models can’t align: With only 64M parameters, BabyJoey lacked the capacity to generalize or hold context, even across short prompts. This confirmed that alignment is not just a data problem - it’s a scale problem.

* Training from scratch is doable: Even on minimal infrastructure, we were able to train a working LLM using local code, community-collected data, and an open-source stack.

* Tone reflects data sometimes too well: The outdated, overly formal voice taught us how strongly style is influenced by corpus era and composition. It emphasized the need for more contemporary, diverse sources.

* Pipeline maturity matters: From tokenization to logging and checkpointing, the process helped us debug critical parts of the infrastructure we’d later need to scale up.
