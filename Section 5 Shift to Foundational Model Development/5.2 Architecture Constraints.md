# 5.2 Architecture Constraints

As we explored what it would take to scale up, we realized that architecture matters just as much as data. Models like Mistral and DeepSeekMoE demonstrated how sparse activation, attention optimization, and high-efficiency routing architectures could drastically shift what was possible even with limited compute.

By contrast, the models we were working with (both open and experimental) were rigid: built for general-purpose English, with no way to adapt their core internals without breaking compatibility or retraining from scratch.

Our experience with BabyJoey reinforced this lesson. While the model was small, the process of building and training it helped us understand how every choice layer depth, hidden size, attention heads, token length impacted behavior, training time, and generalization. These werenâ€™t just technical tweaks they were foundational design decisions we needed to own.
