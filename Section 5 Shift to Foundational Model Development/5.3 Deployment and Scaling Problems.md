# 5.3 Deployment and Scaling Problems

We also began to think more seriously about how these models would be used not just built. Many potential users in education, health, law, and public sector contexts needed models that could run offline, in secure or bandwidth-constrained environments, or on modest hardware.

But most large-scale open models were designed for cloud deployment in inference-optimized clusters not for lightweight, regionally adaptable inference. Fine-tuning couldn’t solve this either. Without the ability to design the model for deployment from the start, we risked building a system that couldn’t run in the places where it mattered most.

Foundational model development gave us the opportunity to shape not just the model’s outputs, but its compute profile, quantization pathway, and compatibility with edge and offline scenarios.
